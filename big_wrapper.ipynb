{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sunpy.map\n",
    "from astropy import units as u\n",
    "import sunpy.coordinates.transformations\n",
    "from sunpy.coordinates import frames\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class harp2noaa():\n",
    "    \"\"\"\n",
    "    A class used to represent an harp2noaa object\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    fsave : str\n",
    "        Filename for base convertor txt\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    update_dataset()\n",
    "        Fetch newest list from jsoc\n",
    "        \n",
    "    harp2noaa(NOAANUM)\n",
    "        Returns HAPRNUM for given NOAA.\n",
    "        Could be int or str\n",
    "    \"\"\"\n",
    "    def __init__(self, fsave='./HARP_TO_NOAA.txt'):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the animal\n",
    "        num_legs : int, optional\n",
    "            The number of legs the animal (default is 4)\n",
    "        \"\"\"\n",
    "        self.fsave = fsave\n",
    "        if not os.path.exists(self.fsave):\n",
    "            print(\"Default HARP_TO_NOAA DOES NOT exists, fetching\")\n",
    "            self.update_dataset()\n",
    "        else:\n",
    "            print(\"Default HARP_TO_NOAA exists\")\n",
    "        print(\"Loading file\")\n",
    "        self._load_dataset()        \n",
    "    \n",
    "    def _load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load dataset into content variable\n",
    "        \"\"\"\n",
    "        with open(self.fsave) as f:\n",
    "            content = [line.rstrip() for line in f]\n",
    "        self.content = content\n",
    "    \n",
    "    def update_dataset(self):\n",
    "        \"\"\"\n",
    "        Wrapper method for download newest harpnum to noaa list\n",
    "        \"\"\"\n",
    "        url = 'http://jsoc.stanford.edu/doc/data/hmi/harpnum_to_noaa/all_harps_with_noaa_ars.txt'\n",
    "        r = requests.get(url)\n",
    "        if r.ok:\n",
    "            print(\"Content fetched, saving...\")\n",
    "            with open(self.fsave, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "                print(\"File saved to {}\".format(self.fsave))\n",
    "            print(\"Loading new dataset\")\n",
    "            self._load_dataset()\n",
    "            \n",
    "    def harp2noaa(self, NOAANUM):\n",
    "        '''\n",
    "        This returns HARP number for provided NOAA region from content file\n",
    "        '''\n",
    "        NOAANUM = str(NOAANUM)\n",
    "        ins = [index for index, string in enumerate(self.content) if NOAANUM in string]\n",
    "        harpnum = [ self.content[x].split(' ')[0] for x in ins ]\n",
    "        if len(harpnum) != 1:\n",
    "            raise Exception(\"Be careful! Your region is over multiple HARPs or it was not found! HARPS: {}\".format(harpnum))\n",
    "        return int(harpnum[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default HARP_TO_NOAA exists\n",
      "Loading file\n",
      "Content fetched, saving...\n",
      "File saved to ./HARP_TO_NOAA.txt\n",
      "Loading new dataset\n",
      "3604\n"
     ]
    }
   ],
   "source": [
    "a = harp2noaa()\n",
    "a.update_dataset()\n",
    "HARPNUM = a.harp2noaa(\"11950\")\n",
    "print(HARPNUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, i dont know how to work with class factories so i cant extend this\n",
    "class process_continuum():\n",
    "    \"\"\"\n",
    "    A class used to represent an process_continuum object.\n",
    "    We use this object to automate continuum images processing\n",
    "    \n",
    "    #### WE ARE NOT LOADING ALL FILES INTO THIS OBJECT, WE ARE GOING ONE BY ONE!!!! ####\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    inpath : str\n",
    "        Path to fits file that we need to process\n",
    "    \n",
    "    opath : str\n",
    "        Path where corrected fits should be saved\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __limb_dark()\n",
    "        limbdarkening coeeficients\n",
    "    \n",
    "    _correct_for_limb()\n",
    "        Function used to correct for limb darkening\n",
    "        \n",
    "    _normalize():\n",
    "        Function used to normalize data\n",
    "        \n",
    "        \n",
    "        \n",
    "    master_wrap\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inpath, overwrite=False):\n",
    "        self.inpath = inpath\n",
    "        self.overwrite = overwrite\n",
    "        # check if input is list\n",
    "        if isinstance(self.inpath, list):\n",
    "            print(\"Dataset detected!\")\n",
    "            #self.outpath = os.path.dirname(self.inpath.replace('raw','processed'))\n",
    "            #print(\"Files will be saved to: {}\".format(self.outpath_dir)) \n",
    "            #Path(os.path.dirname(self.inpath.replace('raw','processed'))).mkdir(parents=True, exist_ok=True)\n",
    "            print(\"Processing ...\")\n",
    "            for in_img in self.inpath:\n",
    "                Path(os.path.dirname(in_img.replace('raw','processed'))).mkdir(parents=True, exist_ok=True)\n",
    "                out = self._master_wrap(in_img)\n",
    "                out.save(in_img.replace('raw','processed'))\n",
    "            print(\"Processing done.\")\n",
    "        elif isinstance(self.inpath, str):\n",
    "            print(\"Single file detected!\")\n",
    "            print(\"Processing ...\")\n",
    "            out = self._master_wrap(self.inpath)\n",
    "            print(\"Processing done.\")\n",
    "            self.outpath = self.inpath.replace('raw','processed')\n",
    "            print(\"File will be saved to: {}\".format(self.outpath)) \n",
    "            Path(os.path.dirname(self.outpath)).mkdir(parents=True, exist_ok=True)\n",
    "            out.save(self.outpath)\n",
    "            print(\"File saved!\")\n",
    "        else:\n",
    "            raise ValueError(\"I cant understand input, should be string or list of strings\")\n",
    "            \n",
    "        \n",
    "    \n",
    "    def __limb_dark(self, r, koef=np.array([0.32519, 1.26432, -1.44591, 1.55723, -0.87415, 0.173333])):\n",
    "        \"\"\"\n",
    "        This function takes r as distance from sun center in units of sun radii and return division factor for correction.\n",
    "        We are making it internal function, because we only need to call it from correct_for_limb function.\n",
    "        \"\"\"\n",
    "        # r is normalized distance from center [0,1]\n",
    "        if len(koef) != 6:\n",
    "            raise ValueErrror(\"koef len should be exactly 6\")\n",
    "        if np.max(r) > 1 or np.min(r) < 0:\n",
    "            raise ValueError(\"r should be in [0,1] range\")\n",
    "        mu = np.sqrt(1-r**2)  # mu = cos(theta)\n",
    "        return koef[0]+koef[1]*mu+koef[2]*mu**2+koef[3]*mu**3+koef[4]*mu**4+koef[5]*mu**5\n",
    "\n",
    "    def _correct_for_limb(self, sunpy_map):\n",
    "        \"\"\"\n",
    "        This function takes sunpy map and removes limb darkening from it\n",
    "        It transfer coordinate mesh to helioprojective coordinate (using data from header)\n",
    "        Calucalates distance from sun center in units of sun radii at the time of observation\n",
    "        Uses limb_dark function with given coeffitiens and divides by that value\n",
    "\n",
    "        Input: sunpy_map (sunpy.map) - input data\n",
    "        Returns: sunpy.map - output data object\n",
    "        \"\"\"\n",
    "        helioproj_limb = sunpy.map.all_coordinates_from_map(sunpy_map).transform_to(\n",
    "            frames.Helioprojective(observer=sunpy_map.observer_coordinate))\n",
    "        rsun_hp_limb = sunpy_map.rsun_obs.value\n",
    "        distance_from_limb = np.sqrt(\n",
    "            helioproj_limb.Tx.value**2+helioproj_limb.Ty.value**2)/rsun_hp_limb\n",
    "        limb_cor_data = sunpy_map.data / self.__limb_dark(distance_from_limb)\n",
    "        return sunpy.map.Map(limb_cor_data, sunpy_map.meta)\n",
    "\n",
    "    # AVERAGE\n",
    "    def _normalize(self, sunpy_map, header_keyword='AVG_F_NO', NBINS=100):\n",
    "        '''\n",
    "        This function normalizes sunpy map\n",
    "        It first creates histogram of data\n",
    "        Finds maximum of histogram and divide whole dataset with that number\n",
    "        This is efectevly normalization to quiet sun\n",
    "\n",
    "        input:  sunpy_map (sunpy.map) - input data\n",
    "                header_keyword (string) - name of header keyword in which maximum of histogram will be written to \n",
    "                                          This allows users to later on, revert to unnormalized image, default is AVG_F_NO\n",
    "                NBINS (int) - How many bins you want for your histogram, default is 100\n",
    "        output: sunpy.map - output data object\n",
    "        '''\n",
    "        weights, bin_edges = np.histogram(\n",
    "            sunpy_map.data.flatten(), bins=NBINS, density=True)\n",
    "        # MAGIC I SAY!\n",
    "        # find maximum of histogram\n",
    "        k = (weights == np.max(weights)).nonzero()[0][0]\n",
    "        # find flux value for maximum of histogram\n",
    "        I_avg = (bin_edges[k+1]+bin_edges[k])/2\n",
    "        # update data\n",
    "        I_new = sunpy_map.data/I_avg\n",
    "        # create new keyword in header\n",
    "        # AVG_F_ON\n",
    "        # AVG_F_EN\n",
    "        sunpy_map.meta[header_keyword] = I_avg\n",
    "        # create new map\n",
    "        return sunpy.map.Map(I_new, sunpy_map.meta)\n",
    "\n",
    "    def _master_wrap(self,fname):\n",
    "        '''\n",
    "        This function is just simple wrapper for all provided functions\n",
    "\n",
    "        input: filename (string) -  fits file path that correction shoud be performed on\n",
    "        output: ofile (string) - string with path to new file\n",
    "        '''\n",
    "        # load data\n",
    "        sunpy_data = sunpy.map.Map(fname)\n",
    "        # correct map for limb\n",
    "        mid_data = self._correct_for_limb(sunpy_data)\n",
    "        # Normalize\n",
    "        mid_data = self._normalize(mid_data, header_keyword='AVG_F_ON')\n",
    "        return mid_data\n",
    "        #mid_data.peek()\n",
    "        # enhance\n",
    "        #mid_data = enhance_wrapper(mid_data)\n",
    "        # normalize again, enhance can make mess with flux\n",
    "        #mid_data = normalize(mid_data, header_keyword='AVG_F_EN')\n",
    "        # Create new filename\n",
    "        \n",
    "        #Create new logic for saving files\n",
    "#         outfile = os.path.basename(filename).replace(\n",
    "#             search_criterium, search_criterium+sufix)\n",
    "#         ofile = os.path.join(output_dir, outfile)\n",
    "        # save map\n",
    "        #mid_data.save(ofile)\n",
    "        #return ofile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     def enhance_wrapper(sunpy_map, depth=5, model=\"keepsize\", activation=\"relu\", ntype=\"intensity\"):\n",
    "#         '''\n",
    "#         This procedures run enhance https://github.com/cdiazbas/enhance (it works only from my fork https://github.com/lzivadinovic/enhance)\n",
    "#         on input sunpy map\n",
    "#         Check source code for explanation of code and input parameters\n",
    "\n",
    "#         input: sunpy_map (sunpy.map) - input data set\n",
    "#         output: sunpy.map - output data object (enhanced)\n",
    "#         '''\n",
    "#         # if rtype is spmap, there is no need for output, it will return sunpy.map object (lzivadinovic/enhance fork - master branch)\n",
    "#         out = enhance(inputFile=sunpy_map, depth=depth, model=model,\n",
    "#                       activation=activation, ntype=ntype, output='1.fits', rtype='spmap')\n",
    "#         out.define_network()\n",
    "#         return out.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export request pending. [id=\"JSOC_20200425_263\", status=2]\n",
      "Waiting for 0 seconds...\n",
      "16 URLs found for download. Full request totalling 7MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11213fd409b1401aaa509e552b96a5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Files Downloaded', max=16.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MY_MAIL='lazar.zivadinovic.994@gmail.com'\n",
    "import os\n",
    "# Lets make jsoc query \n",
    "from sunpy.net import jsoc\n",
    "from sunpy.net import attrs as a\n",
    "from sunpy.time import parse_time\n",
    "#initialize client\n",
    "client = jsoc.JSOCClient()\n",
    "\n",
    "data_root='./data'\n",
    "#DOWNLOAD_PATH_FOR_RAW_DATA\n",
    "download_path=os.path.join(data_root,str(HARPNUM),'raw')\n",
    "\n",
    "###### THIS IS THE REAL QUERY, LETS MAKE SOMETHING SMALLER FOR TESTING ########\n",
    "# Create query\n",
    "resjsoc = client.search(a.jsoc.PrimeKey('HARPNUM', HARPNUM),\n",
    "                        a.jsoc.Series('hmi.sharp_cea_720s'),\n",
    "                        a.jsoc.Segment('Bp') & a.jsoc.Segment('Bt') & \n",
    "                        a.jsoc.Segment('Br') & a.jsoc.Segment('continuum'))#,\n",
    "                        #a.jsoc.Notify(MY_MAIL))\n",
    "\n",
    "#Lets fetch few images for testing\n",
    "#Note that client.jsoc does not support slice, so we need new query\n",
    "#This is a way around it\n",
    "T1 = resjsoc.table['T_REC'][0]\n",
    "T2 = resjsoc.table['T_REC'][3]\n",
    "resjsoc = client.search(a.jsoc.PrimeKey('HARPNUM', HARPNUM),\n",
    "                        a.Time(T1,T2),\n",
    "                        a.jsoc.Series('hmi.sharp_cea_720s'),\n",
    "                        a.jsoc.Segment('Bp') & a.jsoc.Segment('Bt') & \n",
    "                        a.jsoc.Segment('Br') & a.jsoc.Segment('continuum'),\n",
    "                        a.jsoc.Notify(MY_MAIL))\n",
    "\n",
    "result = client.fetch(resjsoc, path=download_path, progress=True, wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/3604/raw/hmi.sharp_cea_720s.3604.20140108_032400_TAI.continuum.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_031200_TAI.Bp.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_032400_TAI.Bt.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_033600_TAI.continuum.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_031200_TAI.Bt.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_034800_TAI.Bt.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_032400_TAI.Bp.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_034800_TAI.continuum.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_031200_TAI.Br.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_033600_TAI.Bp.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_034800_TAI.Bp.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_031200_TAI.continuum.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_033600_TAI.Br.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_032400_TAI.Br.fits', 'data/3604/raw/hmi.sharp_cea_720s.3604.20140108_034800_TAI.Br.fits']\n",
      "Errors:\n",
      "(error(filepath_partial=<function Downloader.enqueue_file.<locals>.filepath at 0x7ff9742340e0>, url='http://jsoc.stanford.edu/SUM66/D1283128204/S00000/hmi.sharp_cea_720s.3604.20140108_033600_TAI.Bt.fits', exception=ClientConnectorError(ConnectionKey(host='jsoc.stanford.edu', port=80, is_ssl=False, ssl=None, proxy=None, proxy_auth=None, proxy_headers_hash=None), ConnectionRefusedError(111, \"Connect call failed ('171.64.103.244', 80)\"))))\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had some errors, here is log\n",
      "[error(filepath_partial=<function Downloader.enqueue_file.<locals>.filepath at 0x7ff9742340e0>, url='http://jsoc.stanford.edu/SUM66/D1283128204/S00000/hmi.sharp_cea_720s.3604.20140108_033600_TAI.Bt.fits', exception=ClientConnectorError(ConnectionKey(host='jsoc.stanford.edu', port=80, is_ssl=False, ssl=None, proxy=None, proxy_auth=None, proxy_headers_hash=None), ConnectionRefusedError(111, \"Connect call failed ('171.64.103.244', 80)\")))]\n",
      "Lets fetch them via requests library\n",
      "Content fetched, saving...\n",
      "File saved to ./data/3604/raw/hmi.sharp_cea_720s.3604.20140108_033600_TAI.Bt.fits\n"
     ]
    }
   ],
   "source": [
    "if result.errors:\n",
    "    print(\"We had some errors, here is log\")\n",
    "    print(result.errors)\n",
    "    print(\"Lets fetch them via requests library\")\n",
    "    for reserr in result.errors:\n",
    "        r = requests.get(reserr.url)\n",
    "        if r.ok:\n",
    "            print(\"Content fetched, saving...\")\n",
    "        path_for_new_img = os.path.join(download_path,reserr.url.split('/')[-1])\n",
    "        with open(path_for_new_img, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "            print(\"File saved to {}\".format(path_for_new_img))\n",
    "    #we should have all results now, lets update results list\n",
    "    result = glob.glob(os.path.join(download_path,'*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/3604/raw/hmi.sharp_cea_720s.3604.20140108_031200_TAI.continuum.fits', './data/3604/raw/hmi.sharp_cea_720s.3604.20140108_032400_TAI.continuum.fits', './data/3604/raw/hmi.sharp_cea_720s.3604.20140108_033600_TAI.continuum.fits', './data/3604/raw/hmi.sharp_cea_720s.3604.20140108_034800_TAI.continuum.fits']\n"
     ]
    }
   ],
   "source": [
    "# Ok, lets process continuum images\n",
    "cont = sorted([ res for res in result if 'continuum' in res])\n",
    "print(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = sunpy.map.Map(cont)\n",
    "\n",
    "# def master_wrap(filename):\n",
    "#     '''\n",
    "#     This function is just simple wrapper for all privided functions\n",
    "    \n",
    "#     input: filename (string) -  fits file path that correction shoud be performed on\n",
    "#     output: ofile (string) - string with path to new file\n",
    "#     '''\n",
    "#     # load data\n",
    "#     sunpy_data = sunpy.map.Map(filename)\n",
    "#     # correct map for limb\n",
    "#     mid_data = correct_for_limb(sunpy_data)\n",
    "#     # Normalize\n",
    "#     mid_data = normalize(mid_data, header_keyword='AVG_F_ON')\n",
    "#     # enhance\n",
    "#     mid_data = enhance_wrapper(mid_data)\n",
    "#     # normalize again, enhance can make mess with flux\n",
    "#     mid_data = normalize(mid_data, header_keyword='AVG_F_EN')\n",
    "#     # Create new filename\n",
    "#     outfile = os.path.basename(filename).replace(\n",
    "#         search_criterium, search_criterium+sufix)\n",
    "#     ofile = os.path.join(output_dir, outfile)\n",
    "#     # save map\n",
    "#     mid_data.save(ofile)\n",
    "#     return ofile\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/3604/raw/hmi.sharp_cea_720s.3604.20140108_031200_TAI.continuum.fits\n"
     ]
    }
   ],
   "source": [
    "#os.path.join(*cont[0].split('/')[0:-2],'processed')\n",
    "print(cont[0])\n",
    "#process_continuum(cont)\n",
    "a=sunpy.map.Map(cont[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_as_mpl_axes',\n",
       " '_base_name',\n",
       " '_coordinate_frame_name',\n",
       " '_data',\n",
       " '_default_carrington_longitude',\n",
       " '_default_dsun',\n",
       " '_default_heliographic_latitude',\n",
       " '_default_heliographic_longitude',\n",
       " '_default_time',\n",
       " '_fix_bitpix',\n",
       " '_fix_date',\n",
       " '_fix_naxis',\n",
       " '_get_cmap_name',\n",
       " '_get_lon_lat',\n",
       " '_mask',\n",
       " '_meta',\n",
       " '_new_instance',\n",
       " '_nickname',\n",
       " '_reference_latitude',\n",
       " '_reference_longitude',\n",
       " '_registry',\n",
       " '_remove_existing_observer_location',\n",
       " '_rotation_matrix_from_crota',\n",
       " '_shift',\n",
       " '_supported_observer_coordinates',\n",
       " '_uncertainty',\n",
       " '_unit',\n",
       " '_validate_meta',\n",
       " '_wcs',\n",
       " 'bottom_left_coord',\n",
       " 'carrington_latitude',\n",
       " 'carrington_longitude',\n",
       " 'center',\n",
       " 'cmap',\n",
       " 'coordinate_frame',\n",
       " 'coordinate_system',\n",
       " 'data',\n",
       " 'date',\n",
       " 'detector',\n",
       " 'dimensions',\n",
       " 'draw_contours',\n",
       " 'draw_grid',\n",
       " 'draw_limb',\n",
       " 'draw_rectangle',\n",
       " 'dsun',\n",
       " 'dtype',\n",
       " 'exposure_time',\n",
       " 'fits_header',\n",
       " 'heliographic_latitude',\n",
       " 'heliographic_longitude',\n",
       " 'instrument',\n",
       " 'is_datasource_for',\n",
       " 'latex_name',\n",
       " 'mask',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'measurement',\n",
       " 'meta',\n",
       " 'min',\n",
       " 'name',\n",
       " 'ndim',\n",
       " 'nickname',\n",
       " 'observatory',\n",
       " 'observer_coordinate',\n",
       " 'peek',\n",
       " 'pixel_to_world',\n",
       " 'plot',\n",
       " 'plot_settings',\n",
       " 'processing_level',\n",
       " 'reference_coordinate',\n",
       " 'reference_pixel',\n",
       " 'resample',\n",
       " 'rotate',\n",
       " 'rotation_matrix',\n",
       " 'rsun_meters',\n",
       " 'rsun_obs',\n",
       " 'save',\n",
       " 'scale',\n",
       " 'shift',\n",
       " 'shifted_value',\n",
       " 'size',\n",
       " 'spatial_units',\n",
       " 'std',\n",
       " 'submap',\n",
       " 'superpixel',\n",
       " 'top_right_coord',\n",
       " 'uncertainty',\n",
       " 'unit',\n",
       " 'wavelength',\n",
       " 'waveunit',\n",
       " 'wcs',\n",
       " 'world_to_pixel']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ondrejov",
   "language": "python",
   "name": "ondrejov"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
